---
layout: post
title: "Bike Share Regression Test"
date: 2024-06-10
---

# Bike Sharing Data Set - Predict Bike Rental Count 

**From Data Science Dojo** 

### Background

- We are looking at the 'daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information'.

- The information was obtained from Data Science Dojo: 
  - https://code.datasciencedojo.com/datasciencedojo/datasets/tree/master/Bike%20Sharing
  

- Which comes from UCI:
  - https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset
  

- I am looking to practice some machine learning techniques.  
- In order to be able to do this, I need to frame the analysis with a question.  
- What might the holders/creators of this data want to understand?

### The Requirement

- In the dataset, there are 3 target variables; the count of hires from registered users; the count of hires from unregistered users and the total count of hires.  
- It strikes me that a bike share company would know least about its unregistered or casual users and may need to want to understand demand from this segment, to ensure they have enough supply of bikes for a given day.
- So our question is:  

**"Can you build a model that predicts demand for any given day, using weather and seasonal information?"**

- Since we are looking to predict a continuous number, this is a regression problem
- The dataset spans two years, so there is sufficient scope to understand seasonality
- Each row of the dataset represents one day

### Import required libraries


```python
# data manipulation
import pandas as pd
import numpy as np
import io
import requests


# data viz
import matplotlib.pyplot as plt
import seaborn as sns
# plt.style.use("fivethirtyeight") # styling the charts

# to convert categorical variables
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer

# data prep
from sklearn.model_selection import train_test_split

# ML algos
from sklearn.linear_model import LinearRegression

# metrics 
from sklearn.metrics import mean_squared_error, mean_absolute_error
```

### Data Dictionary

![Bike%20Sharing%20Data%20Dictionary.PNG](attachment:Bike%20Sharing%20Data%20Dictionary.PNG)

### Import the Data


```python
# Importing our data

url = "https://code.datasciencedojo.com/datasciencedojo/datasets/raw/ca8c1fd3e812d8541c46e37cf27ea4eef5cc56a1/Bike%20Sharing/day.csv"
headers = {'User-Agent': 'Mozilla/5.0'}

response = requests.get(url, headers=headers)
response.raise_for_status()  # Check if the request was successful

# Print the first 500 characters of the response to check the content
print(response.text[:500])

# Assuming the content is correct, read it into a DataFrame
df = pd.read_csv(io.StringIO(response.text))
print(df.head())

```

    instant,dteday,season,yr,mnth,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt
    1,2011-01-01,1,0,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446,331,654,985
    2,2011-01-02,1,0,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539,131,670,801
    3,2011-01-03,1,0,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309,120,1229,1349
    4,2011-01-04,1,0,1,0,2,1,1,0.2,0.212122,0.590435,0.160296,108,1454,1562
    5,2011-01-05,1,0,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869,82,1518,1600
    6,2011-01-06,
       instant      dteday  season  yr  mnth  holiday  weekday  workingday  \
    0        1  2011-01-01       1   0     1        0        6           0   
    1        2  2011-01-02       1   0     1        0        0           0   
    2        3  2011-01-03       1   0     1        0        1           1   
    3        4  2011-01-04       1   0     1        0        2           1   
    4        5  2011-01-05       1   0     1        0        3           1   
    
       weathersit      temp     atemp       hum  windspeed  casual  registered  \
    0           2  0.344167  0.363625  0.805833   0.160446     331         654   
    1           2  0.363478  0.353739  0.696087   0.248539     131         670   
    2           1  0.196364  0.189405  0.437273   0.248309     120        1229   
    3           1  0.200000  0.212122  0.590435   0.160296     108        1454   
    4           1  0.226957  0.229270  0.436957   0.186900      82        1518   
    
        cnt  
    0   985  
    1   801  
    2  1349  
    3  1562  
    4  1600  
    

### Data Inspection


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>instant</th>
      <th>dteday</th>
      <th>season</th>
      <th>yr</th>
      <th>mnth</th>
      <th>holiday</th>
      <th>weekday</th>
      <th>workingday</th>
      <th>weathersit</th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>0.344167</td>
      <td>0.363625</td>
      <td>0.805833</td>
      <td>0.160446</td>
      <td>331</td>
      <td>654</td>
      <td>985</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2011-01-02</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0.363478</td>
      <td>0.353739</td>
      <td>0.696087</td>
      <td>0.248539</td>
      <td>131</td>
      <td>670</td>
      <td>801</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2011-01-03</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0.196364</td>
      <td>0.189405</td>
      <td>0.437273</td>
      <td>0.248309</td>
      <td>120</td>
      <td>1229</td>
      <td>1349</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2011-01-04</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.200000</td>
      <td>0.212122</td>
      <td>0.590435</td>
      <td>0.160296</td>
      <td>108</td>
      <td>1454</td>
      <td>1562</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2011-01-05</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0.226957</td>
      <td>0.229270</td>
      <td>0.436957</td>
      <td>0.186900</td>
      <td>82</td>
      <td>1518</td>
      <td>1600</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.shape
```




    (731, 16)




```python
df.isna().sum()
```




    instant       0
    dteday        0
    season        0
    yr            0
    mnth          0
    holiday       0
    weekday       0
    workingday    0
    weathersit    0
    temp          0
    atemp         0
    hum           0
    windspeed     0
    casual        0
    registered    0
    cnt           0
    dtype: int64




```python
df.duplicated().sum()
```




    0




```python
df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>instant</th>
      <th>season</th>
      <th>yr</th>
      <th>mnth</th>
      <th>holiday</th>
      <th>weekday</th>
      <th>workingday</th>
      <th>weathersit</th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>366.000000</td>
      <td>2.496580</td>
      <td>0.500684</td>
      <td>6.519836</td>
      <td>0.028728</td>
      <td>2.997264</td>
      <td>0.683995</td>
      <td>1.395349</td>
      <td>0.495385</td>
      <td>0.474354</td>
      <td>0.627894</td>
      <td>0.190486</td>
      <td>848.176471</td>
      <td>3656.172367</td>
      <td>4504.348837</td>
    </tr>
    <tr>
      <th>std</th>
      <td>211.165812</td>
      <td>1.110807</td>
      <td>0.500342</td>
      <td>3.451913</td>
      <td>0.167155</td>
      <td>2.004787</td>
      <td>0.465233</td>
      <td>0.544894</td>
      <td>0.183051</td>
      <td>0.162961</td>
      <td>0.142429</td>
      <td>0.077498</td>
      <td>686.622488</td>
      <td>1560.256377</td>
      <td>1937.211452</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.059130</td>
      <td>0.079070</td>
      <td>0.000000</td>
      <td>0.022392</td>
      <td>2.000000</td>
      <td>20.000000</td>
      <td>22.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>183.500000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.337083</td>
      <td>0.337842</td>
      <td>0.520000</td>
      <td>0.134950</td>
      <td>315.500000</td>
      <td>2497.000000</td>
      <td>3152.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>366.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>7.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.498333</td>
      <td>0.486733</td>
      <td>0.626667</td>
      <td>0.180975</td>
      <td>713.000000</td>
      <td>3662.000000</td>
      <td>4548.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>548.500000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>10.000000</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>0.655417</td>
      <td>0.608602</td>
      <td>0.730209</td>
      <td>0.233214</td>
      <td>1096.000000</td>
      <td>4776.500000</td>
      <td>5956.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>731.000000</td>
      <td>4.000000</td>
      <td>1.000000</td>
      <td>12.000000</td>
      <td>1.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>0.861667</td>
      <td>0.840896</td>
      <td>0.972500</td>
      <td>0.507463</td>
      <td>3410.000000</td>
      <td>6946.000000</td>
      <td>8714.000000</td>
    </tr>
  </tbody>
</table>
</div>



- I'd like to look closer at some of these values to understand how they are split and how much data there is for each:


```python
df['yr'].value_counts()
```




    yr
    1    366
    0    365
    Name: count, dtype: int64



- This confirms that our data is split at the daily level
- Year 1 is a leap year


```python
df['weathersit'].value_counts()
```




    weathersit
    1    463
    2    247
    3     21
    Name: count, dtype: int64




```python
df['holiday'].value_counts()
```




    holiday
    0    710
    1     21
    Name: count, dtype: int64




```python
df['workingday'].value_counts()
```




    workingday
    1    500
    0    231
    Name: count, dtype: int64




```python
df['windspeed'].value_counts().sort_values(ascending = False)
```




    windspeed
    0.134954    3
    0.136817    3
    0.110700    3
    0.118792    3
    0.149883    3
               ..
    0.148021    1
    0.376871    1
    0.150500    1
    0.046650    1
    0.154846    1
    Name: count, Length: 650, dtype: int64



- For our EDA, we'll reduce the number of columns
- Our target variables our 'casual', 'registered' and 'cnt'


```python
selected_columns = ['yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']
```


```python
df[selected_columns].describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>yr</th>
      <th>mnth</th>
      <th>holiday</th>
      <th>weekday</th>
      <th>workingday</th>
      <th>weathersit</th>
      <th>temp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
      <td>731.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.500684</td>
      <td>6.519836</td>
      <td>0.028728</td>
      <td>2.997264</td>
      <td>0.683995</td>
      <td>1.395349</td>
      <td>0.495385</td>
      <td>0.627894</td>
      <td>0.190486</td>
      <td>848.176471</td>
      <td>3656.172367</td>
      <td>4504.348837</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.500342</td>
      <td>3.451913</td>
      <td>0.167155</td>
      <td>2.004787</td>
      <td>0.465233</td>
      <td>0.544894</td>
      <td>0.183051</td>
      <td>0.142429</td>
      <td>0.077498</td>
      <td>686.622488</td>
      <td>1560.256377</td>
      <td>1937.211452</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.059130</td>
      <td>0.000000</td>
      <td>0.022392</td>
      <td>2.000000</td>
      <td>20.000000</td>
      <td>22.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.337083</td>
      <td>0.520000</td>
      <td>0.134950</td>
      <td>315.500000</td>
      <td>2497.000000</td>
      <td>3152.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>7.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.498333</td>
      <td>0.626667</td>
      <td>0.180975</td>
      <td>713.000000</td>
      <td>3662.000000</td>
      <td>4548.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>10.000000</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>0.655417</td>
      <td>0.730209</td>
      <td>0.233214</td>
      <td>1096.000000</td>
      <td>4776.500000</td>
      <td>5956.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>12.000000</td>
      <td>1.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>0.861667</td>
      <td>0.972500</td>
      <td>0.507463</td>
      <td>3410.000000</td>
      <td>6946.000000</td>
      <td>8714.000000</td>
    </tr>
  </tbody>
</table>
</div>



- It looks like there might be an issue with 'casual' because the mean and median are quite far apart
- Let's have a visual check...

## EDA

**Univariate Analysis**


```python
df[selected_columns].hist(figsize = (12, 12), grid = True, color= 'darkblue');
```


    
![png](output_27_0.png)
    


- Yes, looks like 'casual' is right skewed
- Our boxplot below reveals many outliers


```python
sns.boxplot(df['casual'])
```




    <Axes: >




    
![png](output_29_1.png)
    



```python
data = {
    'month': df['mnth'],
    'casual': df['casual']
}

box_data = pd.DataFrame(data)
sns.catplot(data=box_data, x='month', y='casual', kind='box')
```

    C:\Users\darre\anaconda3\Lib\site-packages\seaborn\axisgrid.py:118: UserWarning: The figure layout has changed to tight
      self._figure.tight_layout(*args, **kwargs)
    




    <seaborn.axisgrid.FacetGrid at 0x144ded97950>




    
![png](output_30_2.png)
    


- Closer inspection shows that October seems to carry the majority of outliers
- Since our data is split by day, we can interpret this as October having the highest count of unexpectedly high rental days
- Will our regression model account for this?  What effect does weather play? 
- Since linear regression assumes that the residuals (errors) are normally distributed, we should consider a log transform for this target variable before running any regression analysis
- In the meantime, let's have a quick look at the density count for casual riders against, registered riders and total riders


```python
df['casual'].plot.kde(title = 'count of casual riders');
```


    
![png](output_32_0.png)
    



```python
df['registered'].plot.kde(title = 'count of registered riders');
```


    
![png](output_33_0.png)
    



```python
df['cnt'].plot.kde(title = 'count of total riders');
```


    
![png](output_34_0.png)
    


- Since registered riders accounts for the majority of all hires and the distribution is normal, this has resulted in a normal distribution for total bike hires
- We now need to apply a log transformation to our 'casual' data to reduce skewness and make the distribution more symmetric
- This will help to stabilise the variance and make for a more reliable regression model
- We will reverse the log transformation after we have run the model so that we can use the output coefficients in our predictions


```python
casual_log = np.log(df['casual'])
```


```python
df['casual_log'] = casual_log
```


```python
casual_log.plot.kde(title = 'log transformed casual riders');
```


    
![png](output_38_0.png)
    


 - Here's the distribution of our transformed 'casual' data; much better than before
 - Below is our updated dataset with 'casual_log' added.  This will now be our target (y) variable in our model


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>instant</th>
      <th>dteday</th>
      <th>season</th>
      <th>yr</th>
      <th>mnth</th>
      <th>holiday</th>
      <th>weekday</th>
      <th>workingday</th>
      <th>weathersit</th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
      <th>casual_log</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>0.344167</td>
      <td>0.363625</td>
      <td>0.805833</td>
      <td>0.160446</td>
      <td>331</td>
      <td>654</td>
      <td>985</td>
      <td>5.802118</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2011-01-02</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0.363478</td>
      <td>0.353739</td>
      <td>0.696087</td>
      <td>0.248539</td>
      <td>131</td>
      <td>670</td>
      <td>801</td>
      <td>4.875197</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2011-01-03</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0.196364</td>
      <td>0.189405</td>
      <td>0.437273</td>
      <td>0.248309</td>
      <td>120</td>
      <td>1229</td>
      <td>1349</td>
      <td>4.787492</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2011-01-04</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.200000</td>
      <td>0.212122</td>
      <td>0.590435</td>
      <td>0.160296</td>
      <td>108</td>
      <td>1454</td>
      <td>1562</td>
      <td>4.682131</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2011-01-05</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0.226957</td>
      <td>0.229270</td>
      <td>0.436957</td>
      <td>0.186900</td>
      <td>82</td>
      <td>1518</td>
      <td>1600</td>
      <td>4.406719</td>
    </tr>
  </tbody>
</table>
</div>




```python
# plot size
plt.figure(figsize=(12, 6))

# First subplot: total rental count before log transformation
plt.subplot(2, 1, 1)
df['casual'].plot(kind="line", label="count of casual rentals", marker="o")
plt.ylabel("count of rentals")
plt.title("Casual rentals over the period")
plt.legend()

# Second subplot: log of casual rentals
plt.subplot(2, 1, 2)
df['casual_log'].plot(kind="line", label="log of casual rentals", marker="o")
plt.ylabel("log count of rentals")
plt.title("Log transformation of 'Casual' rentals over the period")
plt.legend()

# Adjust layout to prevent overlap
plt.tight_layout()

# show
plt.show()
```


    
![png](output_41_0.png)
    


- The trend in the data is still apparent after the log transform
- Let's have a look at the trend over time for our columns of interest.  This is a good way to quickly spot and understand general patterns in the data (our 'casual' data is in its original format here)


```python
df[selected_columns].plot.line(figsize = (10, 12), subplots=True, marker = 'o');
```


    
![png](output_43_0.png)
    


- We can see how often 'weathersit_3' or bad weather days occur and how often holidays happen
- I wonder if either of these has a relationships with hires. I can imagine a negative relationship with 'weathersit_3' and a positive one for 'holiday'
- We can have a look for these relationships in our bivariate analysis, however we might struggle to see what we're looking for with our data in its current shape 
- For example the level of detail containg specific values for 'weathersit_3' is currently wrapped up the generic 'weathersit' feature.  The same is true for 'mnth' - we cannot understand any specific relationships between say October and hires or October and weather without first flattening out our data.
- This will break out the specific values of our data, such a 'weekday_1' or 'month_10' and give them a binary value
- This makes me a little concerned about multicolinearity creeping in for some of the features, however, One hot encoder can with this (drop='if_binary' - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)

**Bivariate Analysis**


```python
# List of categorical columns to be one-hot encoded
categorical_columns = ['mnth', 'holiday', 'weekday', 'workingday', 'weathersit']

# Initializing OneHotEncoder
encoder = OneHotEncoder(drop='if_binary', sparse_output=False)

# Fitting and transforming the categorical variables
encoded_data = encoder.fit_transform(df[categorical_columns])

# Creating a DataFrame with the encoded data
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenating the encoded variables with the original DataFrame (excluding the original categorical columns)
df_encoded = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)
```


```python
df_encoded.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>instant</th>
      <th>dteday</th>
      <th>season</th>
      <th>yr</th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>...</th>
      <th>weekday_1</th>
      <th>weekday_2</th>
      <th>weekday_3</th>
      <th>weekday_4</th>
      <th>weekday_5</th>
      <th>weekday_6</th>
      <th>workingday_1</th>
      <th>weathersit_1</th>
      <th>weathersit_2</th>
      <th>weathersit_3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>0.344167</td>
      <td>0.363625</td>
      <td>0.805833</td>
      <td>0.160446</td>
      <td>331</td>
      <td>654</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2011-01-02</td>
      <td>1</td>
      <td>0</td>
      <td>0.363478</td>
      <td>0.353739</td>
      <td>0.696087</td>
      <td>0.248539</td>
      <td>131</td>
      <td>670</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2011-01-03</td>
      <td>1</td>
      <td>0</td>
      <td>0.196364</td>
      <td>0.189405</td>
      <td>0.437273</td>
      <td>0.248309</td>
      <td>120</td>
      <td>1229</td>
      <td>...</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2011-01-04</td>
      <td>1</td>
      <td>0</td>
      <td>0.200000</td>
      <td>0.212122</td>
      <td>0.590435</td>
      <td>0.160296</td>
      <td>108</td>
      <td>1454</td>
      <td>...</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2011-01-05</td>
      <td>1</td>
      <td>0</td>
      <td>0.226957</td>
      <td>0.229270</td>
      <td>0.436957</td>
      <td>0.186900</td>
      <td>82</td>
      <td>1518</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>2011-01-06</td>
      <td>1</td>
      <td>0</td>
      <td>0.204348</td>
      <td>0.233209</td>
      <td>0.518261</td>
      <td>0.089565</td>
      <td>88</td>
      <td>1518</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>2011-01-07</td>
      <td>1</td>
      <td>0</td>
      <td>0.196522</td>
      <td>0.208839</td>
      <td>0.498696</td>
      <td>0.168726</td>
      <td>148</td>
      <td>1362</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>2011-01-08</td>
      <td>1</td>
      <td>0</td>
      <td>0.165000</td>
      <td>0.162254</td>
      <td>0.535833</td>
      <td>0.266804</td>
      <td>68</td>
      <td>891</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>2011-01-09</td>
      <td>1</td>
      <td>0</td>
      <td>0.138333</td>
      <td>0.116175</td>
      <td>0.434167</td>
      <td>0.361950</td>
      <td>54</td>
      <td>768</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>2011-01-10</td>
      <td>1</td>
      <td>0</td>
      <td>0.150833</td>
      <td>0.150888</td>
      <td>0.482917</td>
      <td>0.223267</td>
      <td>41</td>
      <td>1280</td>
      <td>...</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 36 columns</p>
</div>



- Let's rename some of those new columns to be a little clearer
- A quick google search shows that 1st Jan 2011 was a Saturday 


```python
df_encoded = df_encoded.rename({"weekday_0": "sun", "weekday_1":'mon', "weekday_2": 'tue', 'weekday_3': 'wed', 'weekday_4': 'thu', 'weekday_5': 'fri', 'weekday_6': 'sat'}, axis = 1)
df_encoded = df_encoded.rename({'mnth_1': 'jan', 'mnth_2': 'feb', 'mnth_3': 'mar', 'mnth_4': 'apr', 'mnth_5': 'may', 'mnth_6': 'jun', 'mnth_7': 'jul', 'mnth_8': 'aug', 'mnth_8': 'aug', 'mnth_9': 'sep', 'mnth_10': 'oct', 'mnth_11': 'nov', 'mnth_12': 'dec'}, axis = 1)
```


```python
df_encoded.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 731 entries, 0 to 730
    Data columns (total 36 columns):
     #   Column        Non-Null Count  Dtype  
    ---  ------        --------------  -----  
     0   instant       731 non-null    int64  
     1   dteday        731 non-null    object 
     2   season        731 non-null    int64  
     3   yr            731 non-null    int64  
     4   temp          731 non-null    float64
     5   atemp         731 non-null    float64
     6   hum           731 non-null    float64
     7   windspeed     731 non-null    float64
     8   casual        731 non-null    int64  
     9   registered    731 non-null    int64  
     10  cnt           731 non-null    int64  
     11  casual_log    731 non-null    float64
     12  jan           731 non-null    float64
     13  feb           731 non-null    float64
     14  mar           731 non-null    float64
     15  apr           731 non-null    float64
     16  may           731 non-null    float64
     17  jun           731 non-null    float64
     18  jul           731 non-null    float64
     19  aug           731 non-null    float64
     20  sep           731 non-null    float64
     21  oct           731 non-null    float64
     22  nov           731 non-null    float64
     23  dec           731 non-null    float64
     24  holiday_1     731 non-null    float64
     25  sun           731 non-null    float64
     26  mon           731 non-null    float64
     27  tue           731 non-null    float64
     28  wed           731 non-null    float64
     29  thu           731 non-null    float64
     30  fri           731 non-null    float64
     31  sat           731 non-null    float64
     32  workingday_1  731 non-null    float64
     33  weathersit_1  731 non-null    float64
     34  weathersit_2  731 non-null    float64
     35  weathersit_3  731 non-null    float64
    dtypes: float64(29), int64(6), object(1)
    memory usage: 205.7+ KB
    


```python
interest_by_day = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun', 'casual_log']
```


```python
sns.pairplot(df_encoded[interest_by_day], diag_kind= 'kde', kind = 'reg');
```

    C:\Users\darre\anaconda3\Lib\site-packages\seaborn\axisgrid.py:118: UserWarning: The figure layout has changed to tight
      self._figure.tight_layout(*args, **kwargs)
    


    
![png](output_52_1.png)
    



```python
columns_interest = ['season', 'holiday_1', 'workingday_1', 'weathersit_1', 'weathersit_2', 'weathersit_3', 'temp', 'hum', 'windspeed', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'casual_log']
```


```python
corr = df_encoded[columns_interest].corr().round(1)
plt.figure(figsize=(20, 15)) 
sns.heatmap(corr, annot = True);
plt.show()
```


    
![png](output_54_0.png)
    


- Here we can identify whether relationships seem to exist 
- It looks like 'weathersit_1' and 'weathersit_2' are highly negatively correlated
- There is also a strong negative correlation between 'weathersit_1' and 'hum'
- With 'weathersit_2' and 'hum', there is also notable correlation (positive)
- Meanwhile, 'temp' seems to have a strong positive correlation with our target variable 'casual_log'
- 'workingday_1' and 'jan' seem to also have notable negative correlations with 'casual_log', which is supported by our pairplot above
- Since 'workingday_1' seems to have a relationship with 'casual_log', we should add the days of the week back into our feature dataset for the regression model build.  They were left out here due to space restrictions
- These results suggest that a fair amount of redundancy is present in some of the weather related features
- For this reason, we will use Lasso regression instead of simple Linear regression
- Lasso regression is useful for feature selection and mitigating the effects of multicollinearity  
- It does do by adding a penalty to the model which can shrink some coefficients to zero effectively removing them 


```python
sns.lmplot(x = "temp", y = "casual_log", data = df_encoded,  order = 1, markers= ['v'], ci = False);
sns.lmplot(x = "workingday_1", y = "casual_log", data = df_encoded, ci = False, order = 1, markers= ['o']);
```

    C:\Users\darre\anaconda3\Lib\site-packages\seaborn\axisgrid.py:118: UserWarning: The figure layout has changed to tight
      self._figure.tight_layout(*args, **kwargs)
    C:\Users\darre\anaconda3\Lib\site-packages\seaborn\axisgrid.py:118: UserWarning: The figure layout has changed to tight
      self._figure.tight_layout(*args, **kwargs)
    


    
![png](output_56_1.png)
    



    
![png](output_56_2.png)
    


### Prep for Regression

- Some final updates to our feature dataset


```python
df_final = df_encoded.drop(['instant', 'dteday', 'season', 'yr', 'atemp', 'hum', 'casual', 'registered', 'cnt', 'casual_log'], axis=1)
```


```python
df_final.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 731 entries, 0 to 730
    Data columns (total 26 columns):
     #   Column        Non-Null Count  Dtype  
    ---  ------        --------------  -----  
     0   temp          731 non-null    float64
     1   windspeed     731 non-null    float64
     2   jan           731 non-null    float64
     3   feb           731 non-null    float64
     4   mar           731 non-null    float64
     5   apr           731 non-null    float64
     6   may           731 non-null    float64
     7   jun           731 non-null    float64
     8   jul           731 non-null    float64
     9   aug           731 non-null    float64
     10  sep           731 non-null    float64
     11  oct           731 non-null    float64
     12  nov           731 non-null    float64
     13  dec           731 non-null    float64
     14  holiday_1     731 non-null    float64
     15  sun           731 non-null    float64
     16  mon           731 non-null    float64
     17  tue           731 non-null    float64
     18  wed           731 non-null    float64
     19  thu           731 non-null    float64
     20  fri           731 non-null    float64
     21  sat           731 non-null    float64
     22  workingday_1  731 non-null    float64
     23  weathersit_1  731 non-null    float64
     24  weathersit_2  731 non-null    float64
     25  weathersit_3  731 non-null    float64
    dtypes: float64(26)
    memory usage: 148.6 KB
    


```python
# splitting the data 
X = df_final
y = df_encoded["casual_log"]
```


```python
y
```




    0      5.802118
    1      4.875197
    2      4.787492
    3      4.682131
    4      4.406719
             ...   
    726    5.509388
    727    6.467699
    728    5.068904
    729    5.897154
    730    6.084499
    Name: casual_log, Length: 731, dtype: float64




```python
#train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)
```

### Model building

- First, we'll need to import some new libraries


```python
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
```


```python
# Define the pipeline
pipe = Pipeline([
    ('transform', StandardScaler()),
    ('lasso', Lasso())
])
```


```python
# These are our possible alpha values
param_grid = {
    'lasso__alpha': [0.1, 1, 10, 100, 1000]
}
```


```python
# Use GridSearchCV to perform cross-validation
grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='r2')  # 5-fold cross-validation
grid_search.fit(X_train, y_train)
```




<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=5,
             estimator=Pipeline(steps=[(&#x27;transform&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: [0.1, 1, 10, 100, 1000]},
             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=5,
             estimator=Pipeline(steps=[(&#x27;transform&#x27;, StandardScaler()),
                                       (&#x27;lasso&#x27;, Lasso())]),
             param_grid={&#x27;lasso__alpha&#x27;: [0.1, 1, 10, 100, 1000]},
             scoring=&#x27;r2&#x27;)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" ><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;transform&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" ><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" ><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">Lasso</label><div class="sk-toggleable__content"><pre>Lasso()</pre></div></div></div></div></div></div></div></div></div></div></div></div>




```python
# Best parameters and best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
```


```python
print("Best alpha:", best_params['lasso__alpha'])
print("Best cross-validated score:", grid_search.best_score_)
```

    Best alpha: 0.1
    Best cross-validated score: 0.6952749287090226
    


```python
# Predict on training set
y_train_pred_log = best_model.predict(X_train)
y_train_pred = np.exp(y_train_pred_log) # Transform predictions back to the original scale

# Predict on test set
y_test_pred_log = best_model.predict(X_test)
y_test_pred = np.exp(y_pred_log) # Transform predictions back to the original scale
```


```python
# Evaluate the model
r2_train_log = best_model.score(X_train, y_train)  # R^2 on the test set (log-transformed)
r2_test_log = best_model.score(X_test, y_test)  # R^2 on the test set (log-transformed)
mse = mean_squared_error(np.exp(y_test), y_pred)  # MSE on the original scale
print("Training set R^2 (log scale):", r2_train_log)
print("Test set R^2 (log scale):", r2_test_log)
print("Mean Squared Error on test set:", mse)
```

    Training set R^2 (log scale): 0.7097275111142798
    Test set R^2 (log scale): 0.7412656309592804
    Mean Squared Error on test set: 175439.9511577715
    

- Approximately 74% of the variability in the log-transformed target variable is explained by the model on the test data 
- This is slightly higher than the training R², suggesting that the model generalizes well to unseen data and is not overfitting
- However, given that the MSE is quite large, it suggests that there are significant errors in the predictions, possibly due to high variance or outliers
- The scatter plot below shows how the data well each predicted data point in our training and test datasets was captured


```python
# Plot for training data
plt.figure(figsize=(12, 6))

# Scatter plot for training set
plt.subplot(1, 2, 1)
plt.scatter(np.exp(y_train), y_train_pred, alpha=0.3)
plt.plot([min(np.exp(y_train)), max(np.exp(y_train))], [min(np.exp(y_train)), max(np.exp(y_train))], color='red', linestyle='--')
plt.xlabel('Actual Values (Training set)')
plt.ylabel('Predicted Values (Training set)')
plt.title('Training Set: Actual vs Predicted')

# Scatter plot for test set
plt.subplot(1, 2, 2)
plt.scatter(np.exp(y_test), y_pred, alpha=0.3)
plt.plot([min(np.exp(y_test)), max(np.exp(y_test))], [min(np.exp(y_test)), max(np.exp(y_test))], color='red', linestyle='--')
plt.xlabel('Actual Values (Test set)')
plt.ylabel('Predicted Values (Test set)')
plt.title('Test Set: Actual vs Predicted')

plt.tight_layout()
plt.show()

```


    
![png](output_75_0.png)
    


- The closer the data point to the dotted line, the more accurate the prediction
- We can see that the level of dispersion increases with higher value points
- This may be due to the outliers in the original data

### Feature Selection

- Let's have a look at which features were included and which were dropped by the model


```python
# Get the feature names (assuming X is a DataFrame; otherwise, replace with appropriate feature names)
feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'Feature {i}' for i in range(X_train.shape[1])]

# Get the coefficients from the best Lasso model
coefficients = best_model.named_steps['lasso'].coef_

# Create a DataFrame to hold the feature names and their corresponding coefficients
coeff_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients
})

# Identify dropped features (coefficients that are zero)
dropped_features = coeff_df[coeff_df['Coefficient'] == 0]['Feature'].tolist()

# Features selected by Lasso
selected_features = coeff_df[coeff_df['Coefficient'] != 0]['Feature'].tolist()

# Display the coefficients and dropped features
print("Feature Coefficients:")
print(coeff_df)
print("\nDropped Features:")
print(dropped_features)
print("\nSelected Features:")
print(selected_features)
```

    Feature Coefficients:
             Feature  Coefficient
    0           temp     0.461420
    1      windspeed    -0.017907
    2            jan    -0.171440
    3            feb    -0.085489
    4            mar     0.000000
    5            apr     0.007750
    6            may     0.000000
    7            jun     0.000000
    8            jul    -0.000000
    9            aug    -0.000000
    10           sep     0.000000
    11           oct     0.000000
    12           nov     0.000000
    13           dec    -0.013524
    14     holiday_1    -0.000000
    15           sun     0.000000
    16           mon    -0.000000
    17           tue    -0.000000
    18           wed    -0.000000
    19           thu    -0.000000
    20           fri     0.000000
    21           sat     0.000000
    22  workingday_1    -0.308452
    23  weathersit_1     0.067107
    24  weathersit_2    -0.000000
    25  weathersit_3    -0.186852
    
    Dropped Features:
    ['mar', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'holiday_1', 'sun', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'weathersit_2']
    
    Selected Features:
    ['temp', 'windspeed', 'jan', 'feb', 'apr', 'dec', 'workingday_1', 'weathersit_1', 'weathersit_3']
    

### Predicting for a given day, given weather and seasonal information

- We can now use the intercept and the coefficients from the model to predict demand for any given day, given weather and seasonal inputs 


```python
# Filter out the features with non-zero coefficients
non_zero_coeff_df = coeff_df[coeff_df['Coefficient'] != 0]

# Convert the DataFrame to a dictionary of non-zero coefficients
non_zero_coeff_dict = non_zero_coeff_df.set_index('Feature')['Coefficient'].to_dict()

# Assuming you have the intercept from the trained model
intercept = best_model.named_steps['lasso'].intercept_

# Example DataFrame with the features for days you want to predict
data = {
    "temp": [0.2, 0.3],  # Example temp values
    "windspeed": [0.1, 0.15],  # Example windspeed values
    "jan": [0, 1],
    "feb": [0, 0],
    "mar": [0, 0],
    "apr": [0, 0],
    "may": [0, 0],
    "jun": [0, 0],
    "jul": [0, 0],
    "aug": [0, 0],
    "sep": [0, 0],
    "oct": [0, 0],
    "nov": [0, 0],
    "dec": [1, 0],  # December for the first row, not December for the second
    "holiday_1": [0, 0],
    "mon": [1, 0],
    "tue": [0, 0],
    "wed": [0, 0],
    "thu": [0, 0],
    "fri": [0, 0],
    "sat": [0, 0],
    "sun": [0, 1],  
    "workingday_1": [1, 0],
    "weathersit_1": [0, 1],  # Different weather situations
    "weathersit_2": [1, 0],
    "weathersit_3": [0, 0]
}

df_to_predict = pd.DataFrame(data)

# Function to predict demand for a row using non-zero coefficients
# def predict_demand_lasso(row, coefficients, intercept):
#     return intercept + sum(coefficients.get(feature, 0) * row[feature] for feature in row.index)
def predict_demand_lasso(row, coefficients, intercept, log_transformed=False):
    prediction = intercept + sum(coefficients.get(feature, 0) * row[feature] for feature in row.index)
    if log_transformed:
        prediction = np.exp(prediction)  # Convert back to the original scale
    return prediction


# Apply the function to each row to get the predicted demand
df_to_predict['predicted_demand'] = df_to_predict.apply(predict_demand_lasso, axis=1, coefficients=non_zero_coeff_dict, intercept=intercept, log_transformed=True)

print(df_to_predict)

```

       temp  windspeed  jan  feb  mar  apr  may  jun  jul  aug  ...  wed  thu  \
    0   0.2       0.10    0    0    0    0    0    0    0    0  ...    0    0   
    1   0.3       0.15    1    0    0    0    0    0    0    0  ...    0    0   
    
       fri  sat  sun  workingday_1  weathersit_1  weathersit_2  weathersit_3  \
    0    0    0    0             1             0             1             0   
    1    0    0    1             0             1             0             0   
    
       predicted_demand  
    0        463.887378  
    1        603.370301  
    
    [2 rows x 27 columns]
    


```python

```
